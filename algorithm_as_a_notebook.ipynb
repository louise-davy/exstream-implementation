{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stumpy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "FOLDERS = [\"folder_1\", \"folder_2\"]\n",
    "SEL_FOLDER = FOLDERS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f.split(\".\")[0] for f in os.listdir(f\"{DATA_FOLDER}/{SEL_FOLDER}\")]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_FILENAME = \"labels\"\n",
    "labels = pd.read_csv(f\"{DATA_FOLDER}/{SEL_FOLDER}/{LABEL_FILENAME}.csv\", index_col=0)\n",
    "train_files = [f for f in files if f != LABEL_FILENAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2anotype = {}\n",
    "for file in train_files:\n",
    "    if file.startswith(\"1\"):\n",
    "        file2anotype[file] = \"bursty input\"\n",
    "    elif file.startswith(\"2\"):\n",
    "        file2anotype[file] = \"stalled input\"\n",
    "    elif file.startswith(\"3\"):\n",
    "        file2anotype[file] = \"CPU contention\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = {}\n",
    "anos = {}\n",
    "\n",
    "for file in train_files:\n",
    "    train_file = pd.read_csv(f\"{DATA_FOLDER}/{SEL_FOLDER}/{file}.csv\", index_col=0)\n",
    "    train_file[\"original_file\"] = file\n",
    "    train_file[\"timestamp\"] = train_file.index\n",
    "    label_file = labels.loc[labels[\"trace_id\"] == file, :]\n",
    "    for i in label_file.index:\n",
    "        ano_id = label_file.loc[i, \"ano_id\"]\n",
    "        selection_ref = train_file.loc[\n",
    "            (train_file[\"timestamp\"] >= label_file[\"ref_start\"][i])\n",
    "            & (train_file[\"timestamp\"] < label_file[\"ref_end\"][i]),\n",
    "            :,\n",
    "        ].copy()\n",
    "        selection_ref[\"ano_id\"] = ano_id\n",
    "        selection_ref[\"type_data\"] = 0\n",
    "        selection_ano = train_file.loc[\n",
    "            (train_file[\"timestamp\"] >= label_file[\"ano_start\"][i])\n",
    "            & (train_file[\"timestamp\"] <= label_file[\"ano_end\"][i]),\n",
    "            :,\n",
    "        ].copy()\n",
    "        selection_ano[\"ano_id\"] = ano_id\n",
    "        selection_ano[\"type_data\"] = 1\n",
    "        refs[f\"{file2anotype[file]}_{file}_{i}\"] = selection_ref\n",
    "        anos[f\"{file2anotype[file]}_{file}_{i}\"] = selection_ano\n",
    "\n",
    "assert refs.keys() == anos.keys()\n",
    "refs = pd.concat(refs).droplevel(1)\n",
    "anos = pd.concat(anos).droplevel(1)\n",
    "refs.shape, anos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursty_refs = refs[refs.index.str.startswith(\"bursty\")]\n",
    "bursty_anos = anos[anos.index.str.startswith(\"bursty\")]\n",
    "stalled_refs = refs[refs.index.str.startswith(\"stalled\")]\n",
    "stalled_anos = anos[anos.index.str.startswith(\"stalled\")]\n",
    "cpu_refs = refs[refs.index.str.startswith(\"CPU\")]\n",
    "cpu_anos = anos[anos.index.str.startswith(\"CPU\")]\n",
    "\n",
    "bursty_refs.shape, bursty_anos.shape, stalled_refs.shape, stalled_anos.shape, cpu_refs.shape, cpu_anos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_entropy(nb_ts_a: list, nb_ts_r: list) -> float:\n",
    "    \"\"\"Calculate the class entropy of a feature, which is the information\n",
    "    needed to describe the class distributions between two time serie.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nb_ts_a : list\n",
    "        Number of observations inside a time series belonging to the abnormal class.\n",
    "    nb_ts_r : list\n",
    "        Number of observations inside a time series belonging to the reference class.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The class entropy.\n",
    "    \"\"\"\n",
    "    if nb_ts_a == 0 or nb_ts_r == 0:\n",
    "        raise ValueError(\n",
    "            f\"One of the time series is empty. Len of TSA is {nb_ts_a} and len of TSR is {nb_ts_r}.\"\n",
    "        )\n",
    "    p_a = nb_ts_a / (nb_ts_a + nb_ts_r)\n",
    "    p_r = nb_ts_r / (nb_ts_a + nb_ts_r)\n",
    "    h_class = p_a * np.log2(p_a) + p_r * np.log2(p_r)\n",
    "    return h_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_observations_if_duplicates(\n",
    "    sorted_values: pd.DataFrame, feature\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Shuffle the observations if there are duplicates in the sorted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sorted_values : pd.DataFrame\n",
    "        The sorted values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The sorted values with shuffled values for duplicates.\n",
    "    \"\"\"\n",
    "\n",
    "    # On récupère le nombre de références et d'anomalies pour chaque modalité\n",
    "    value_type_to_count = sorted_values.groupby(feature).value_counts().to_dict()\n",
    "\n",
    "    # On récupère le nombre de valeurs distinctes pour chaque modalité (soit 1 lorsque pas de doublons, soit 2, lorsqu'il y a des doublons)\n",
    "    value_to_count_distinct = (\n",
    "        sorted_values.drop_duplicates().groupby(feature).count().to_dict()[\"type_data\"]\n",
    "    )\n",
    "\n",
    "    # On récupère les modalités distinctes (les différentes valeurs prises par la feature)\n",
    "    modalities = set(sorted_values[feature].tolist())\n",
    "    # En fait, ce qui est un peu bizarre c'est qu'on a des valeurs continues, mais là on va les traiter comme des valeurs discrètes :\n",
    "    # Par exemple si on considère une colonne qui prend les valeurs 501.03, 501.03, 502.4, 502.4, 505.0, on itère sur 501.03, 502.4, 505.0\n",
    "\n",
    "    # On parcourt chaque modalité\n",
    "    for modality in modalities:\n",
    "        # On récupère le premier type de données observé (ano ou ref, donc 1 ou 0) (ça nous sera utile plus tard)\n",
    "        last_type_data = sorted_values.loc[\n",
    "            sorted_values[feature] == modality, \"type_data\"\n",
    "        ].tolist()[0]\n",
    "\n",
    "        # Cas où il n'y a pas de doublons\n",
    "        if value_to_count_distinct[modality] == 1:\n",
    "            # On ne fait rien\n",
    "            continue\n",
    "\n",
    "        # Cas où il y a des doublons\n",
    "        else:\n",
    "            # On va shuffle dans le pire ordre possible\n",
    "\n",
    "            # D'abord on instancie les variables nécessaires\n",
    "            list_values = []\n",
    "            nb_refs = value_type_to_count[(modality, 0)]\n",
    "            nb_anos = value_type_to_count[(modality, 1)]\n",
    "            nb_total = nb_refs + nb_anos\n",
    "            # Hop maintenant c'est parti pour le shuffle\n",
    "\n",
    "            # Cas où il n'y a pas le même nombre de références et d'anomalies (cas le plus chiant)\n",
    "            if nb_refs != nb_anos:\n",
    "                # On instancie de nouveau des variables nécessaires\n",
    "                biggest = int(\n",
    "                    nb_refs < nb_anos\n",
    "                )  # 1 si on a plus d'anomalies que de références, 0 sinon\n",
    "                smallest = int(\n",
    "                    nb_refs > nb_anos\n",
    "                )  # 1 si on a plus de références que d'anomalies, 0 sinon\n",
    "                nb_smallest = min(\n",
    "                    nb_refs, nb_anos\n",
    "                )  # Nombre de fois où on va mettre la valeur la moins représentée\n",
    "\n",
    "                # On commence par mettre la valeur la plus représentée partout\n",
    "                list_values = [biggest] * nb_total\n",
    "\n",
    "                # Puis on cherche si le dernier type de donnée observé est le plus représenté ou le moins représenté pour savoir où commencer\n",
    "                start_smallest = 0 if smallest != last_type_data else 1\n",
    "\n",
    "                # On parcourt la liste 2 par 2 pour mettre la valeur la moins représentée\n",
    "                for i in range(start_smallest, nb_smallest * 2, 2):\n",
    "                    list_values[i] = smallest\n",
    "\n",
    "            # Cas où il y a le même nombre de références et d'anomalies (cas le plus simple)\n",
    "            else:\n",
    "                # On parcourt le nombre total d'observations\n",
    "                for i in range(nb_total):\n",
    "                    # On alterne entre 0 et 1 en commençant par la valeur opposée à la dernière valeur observée\n",
    "                    list_values.append(abs(last_type_data - i % 2 - 1))\n",
    "\n",
    "            # On récupère le dernier type de donnée observé (toujours 1 ou 0)\n",
    "            last_type_data = sorted_values.loc[\n",
    "                sorted_values[feature] == modality, \"type_data\"\n",
    "            ].tolist()[-1]\n",
    "\n",
    "        # On vérifie que la longueur de la liste est bien égale au nombre d'observations pour la modalité\n",
    "        assert (\n",
    "            len(list_values)\n",
    "            == sorted_values[sorted_values[feature] == modality].shape[0]\n",
    "        ), f\"Len of list_values {len(list_values)} is not equal to the number of observations for the modality {sorted_values[sorted_values[feature]==modality].shape[0]}.\"\n",
    "        # On met à jour le type de donnée pour la modalité\n",
    "        sorted_values.loc[sorted_values[feature] == modality, \"type_data\"] = list_values\n",
    "\n",
    "    return sorted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_entropy(shuffled_values: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the segmentation entropy of a feature.\n",
    "\n",
    "    Parameters:\n",
    "    shuffled_values (pd.DataFrame): A DataFrame containing the shuffled values.\n",
    "\n",
    "    Returns:\n",
    "    float: The segmentation entropy of the feature.\n",
    "    \"\"\"\n",
    "    # On récupère la time serie\n",
    "    ts = shuffled_values[\"type_data\"].tolist()\n",
    "\n",
    "    # Stocke la première valeur de la liste\n",
    "    past_value = ts[0]\n",
    "\n",
    "    # Liste pour stocker les valeurs à l'intérieur d'un segment\n",
    "    values_inside_segment = []\n",
    "\n",
    "    # Variable pour stocker la segmentation entropy\n",
    "    segmentation_ent = 0.0\n",
    "\n",
    "    # Parcourt chaque valeur dans la time serie\n",
    "    for value in ts:\n",
    "        # Si la valeur est différente de la valeur précédente\n",
    "        if value != past_value:\n",
    "            # On a un nouveau segment, il faut calculer l'entropie de segmentation partielle du précédent segment\n",
    "            pi = len(values_inside_segment) / shuffled_values.shape[0]\n",
    "            segmentation_ent += pi * np.log(1 / pi)\n",
    "\n",
    "            # On réinitialise la liste des valeurs à l'intérieur du segment avec la nouvelle valeur\n",
    "            values_inside_segment = [value]\n",
    "        else:\n",
    "            # On stocke les valeurs à l'intérieur du segment tant qu'un nouveau segment n'est pas créé\n",
    "            values_inside_segment.append(value)\n",
    "\n",
    "        # On met à jour la valeur précédente avec la valeur actuelle\n",
    "        past_value = value\n",
    "\n",
    "    return segmentation_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_reward_function(refs, anos):\n",
    "    \"\"\"\n",
    "    Calculates the reward function for a single feature based on the reference data and the annotated data.\n",
    "\n",
    "    Parameters:\n",
    "    refs (pandas.DataFrame): The reference data.\n",
    "    anos (pandas.DataFrame): The abnormal data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the single reward function for each feature.\n",
    "    \"\"\"\n",
    "    distances = {}\n",
    "    # On calcule la class entropy\n",
    "    class_ent = class_entropy(refs.shape[0], anos.shape[0])\n",
    "    # On calcule la segmentation entropy pour chaque feature sauf type_data\n",
    "    for feature in [col for col in refs.columns if col != \"type_data\"]:\n",
    "        # On concatène les références et les anomalies pour la feature\n",
    "        all_values = pd.concat(\n",
    "            [refs[[feature, \"type_data\"]], anos[[feature, \"type_data\"]]]\n",
    "        )\n",
    "        # On trie les valeurs par feature puis par type_data\n",
    "        sorted_values = all_values.sort_values(by=[feature, \"type_data\"])\n",
    "        # On shuffle les valeurs si on a des doublons\n",
    "        shuffled_values = shuffle_observations_if_duplicates(sorted_values, feature)\n",
    "        # On calcule la segmentation entropy\n",
    "        segmentation_ent = segmentation_entropy(shuffled_values)\n",
    "        # On calcule la single reward function\n",
    "        distance = class_ent / segmentation_ent\n",
    "        # On stocke la single reward function dans le dictionnaire\n",
    "        distances[feature] = distance\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def single_reward_function_check(refs, anos):\n",
    "    distances = {}\n",
    "    segmentations_ents = {}\n",
    "    class_ent = class_entropy(refs.shape[0], anos.shape[0])\n",
    "    for feature in refs.columns[:-4]:\n",
    "        all_values = pd.concat([refs[[feature, \"type_data\"]], anos[[feature, \"type_data\"]]])\n",
    "        sorted_values = all_values.sort_values(by=[feature, \"type_data\"])\n",
    "        shuffled_values = shuffle_observations_if_duplicates(sorted_values, feature)\n",
    "        segmentation_ent = segmentation_entropy(shuffled_values)\n",
    "        distance = class_ent / segmentation_ent\n",
    "        distances[feature] = distance\n",
    "        segmentations_ents[feature] = segmentation_ent\n",
    "    return distances, segmentations_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursty_df = pd.concat([bursty_refs, bursty_anos])  # .iloc[:, :-4]\n",
    "stalled_df = pd.concat([stalled_refs, stalled_anos])  # .iloc[:, :-4]\n",
    "cpu_df = pd.concat([cpu_refs, cpu_anos])  # .iloc[:, :-4]\n",
    "\n",
    "dfs = [bursty_df, stalled_df, cpu_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(df, correlation_threshold=0.9):\n",
    "    # Step 1: Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Step 2: Create a graph based on pairwise correlations\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes (features) to the graph\n",
    "    G.add_nodes_from(correlation_matrix.columns)\n",
    "\n",
    "    # Add edges between nodes if the correlation exceeds a threshold\n",
    "    for i in range(len(correlation_matrix.columns[:-4])):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "                G.add_edge(correlation_matrix.columns[i], correlation_matrix.columns[j])\n",
    "\n",
    "    # Step 3: Extract clusters from the graph\n",
    "    clusters = list(nx.connected_components(G))\n",
    "\n",
    "    # Step 4: Select one representative feature from each cluster\n",
    "    selected_features = [cluster.pop() for cluster in clusters]\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features = []\n",
    "\n",
    "for df in dfs:\n",
    "    filtered_features.append(remove_correlated_features(df))\n",
    "\n",
    "print(\", \".join([str(len(feature)) for feature in filtered_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_filtering(refs, anos, cols):\n",
    "    new_cols = []\n",
    "    refs_df = refs[cols]\n",
    "    anos_df = anos[cols]\n",
    "    cols_to_visit = list(anos_df.columns[:-4])\n",
    "    for ano in anos_df.index.unique():\n",
    "        for col in cols_to_visit:\n",
    "            pattern = anos_df.loc[ano, col]\n",
    "            ts = refs_df.loc[:, col]\n",
    "            matches = stumpy.match(pattern, ts, max_distance=28.0)\n",
    "            if not list(matches):\n",
    "                if col not in new_cols:\n",
    "                    new_cols.append(col)\n",
    "            else:\n",
    "                cols_to_visit.remove(col)\n",
    "                print(f\"Found {len(matches)} match(es) for {col} in ano {ano}\")\n",
    "\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_features_bursty = false_positive_filtering(\n",
    "    bursty_refs, bursty_anos, filtered_features[0]\n",
    ")\n",
    "new_filtered_features_stalled = false_positive_filtering(\n",
    "    stalled_refs, stalled_anos, filtered_features[1]\n",
    ")\n",
    "new_filtered_features_cpu = false_positive_filtering(\n",
    "    cpu_refs, cpu_anos, filtered_features[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_features_cpu, new_filtered_features_bursty, new_filtered_features_stalled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward leap filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate single reward function for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_bursty = single_reward_function(\n",
    "    bursty_refs.loc[:, new_filtered_features_bursty + [\"type_data\"]],\n",
    "    bursty_anos.loc[:, new_filtered_features_bursty + [\"type_data\"]],\n",
    ")\n",
    "\n",
    "\n",
    "distances_stalled = single_reward_function(\n",
    "    stalled_refs.loc[:, new_filtered_features_stalled + [\"type_data\"]],\n",
    "    stalled_anos.loc[:, new_filtered_features_stalled + [\"type_data\"]],\n",
    ")\n",
    "\n",
    "\n",
    "distances_cpu = single_reward_function(\n",
    "    cpu_refs.loc[:, new_filtered_features_cpu + [\"type_data\"]],\n",
    "    cpu_anos.loc[:, new_filtered_features_cpu + [\"type_data\"]],\n",
    ")\n",
    "\n",
    "distances_bursty, distances_stalled, distances_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifications (not needed inside reel code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = distances_bursty.copy()\n",
    "\n",
    "biggest_entropy = sorted(distances.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "smallest_entropy = sorted(distances.items(), key=lambda x: x[1], reverse=False)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i, col in enumerate([smallest_entropy, biggest_entropy]):\n",
    "    axs[i].hist(refs[col].astype(float).tolist(), label=\"Reference\", alpha=0.5)\n",
    "    axs[i].hist(anos[col].astype(float).tolist(), label=\"Anomaly\", alpha=0.5)\n",
    "    axs[i].ticklabel_format(useOffset=False)\n",
    "    axs[i].set_title(col, fontsize=6)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    int(len(refs.columns[:-4]) / 2), 2, figsize=(10, 2 * len(refs.columns[:-4]))\n",
    ")\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i, item in enumerate(sorted(distances.items(), key=lambda x: x[1])):\n",
    "    col = item[0]\n",
    "    axs[i].hist(refs[col].astype(float).tolist(), label=\"Reference\", alpha=0.5)\n",
    "    axs[i].hist(anos[col].astype(float).tolist(), label=\"Anomaly\", alpha=0.5)\n",
    "    axs[i].ticklabel_format(style=\"plain\")\n",
    "\n",
    "    axs[i].set_title(col, fontsize=6)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_features_bursty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing final explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in train_files:\n",
    "    ano_ids = list(labels.loc[labels[\"trace_id\"] == file, \"ano_id\"])\n",
    "    for ano_id in ano_ids:\n",
    "        selected_ref = refs.loc[\n",
    "            (refs[\"ano_id\"] == ano_id) & (refs[\"original_file\"] == file), :\n",
    "        ]\n",
    "        selected_ano = anos.loc[\n",
    "            (anos[\"ano_id\"] == ano_id) & (anos[\"original_file\"] == file), :\n",
    "        ]\n",
    "\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
