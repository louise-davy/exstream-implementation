{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stumpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "FOLDERS = [\"folder_1\", \"folder_2\"]\n",
    "SEL_FOLDER = FOLDERS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(data_folder: str, selected_folder: str, label_filename: str):\n",
    "    files = [f.split(\".\") for f in os.listdir(f\"{data_folder}/{selected_folder}\")]\n",
    "    labels = pd.read_csv(\n",
    "        f\"{data_folder}/{selected_folder}/{label_filename}.csv\", index_col=0\n",
    "    )\n",
    "    train_files = [file for file in files if file != label_filename]\n",
    "\n",
    "    return train_files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_files_to_anomaly_type(train_files: list) -> dict:\n",
    "    from_files_to_anomaly_type = {}\n",
    "\n",
    "    for file in train_files:\n",
    "        if file.startswith(\"1\"):\n",
    "            from_files_to_anomaly_type[file] = \"bursty input\"\n",
    "        elif file.startswith(\"2\"):\n",
    "            from_files_to_anomaly_type[file] = \"stalled input\"\n",
    "        elif file.startswith(\"3\"):\n",
    "            from_files_to_anomaly_type[file] = \"CPU contention\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown file {file}.\")\n",
    "\n",
    "    return from_files_to_anomaly_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_anomalies_and_references(\n",
    "    train_files: list,\n",
    "    labels: pd.DataFrame,\n",
    "    data_folder: str,\n",
    "    selected_folder: str,\n",
    "    from_files_to_anomaly_type: dict,\n",
    ") -> (dict, dict):\n",
    "    references = {}\n",
    "    anomalies = {}\n",
    "\n",
    "    for filename in train_files:\n",
    "        train_file = pd.read_csv(f\"{data_folder}/{selected_folder}/{filename}.csv\")\n",
    "        train_file[\"original_filename\"] = filename\n",
    "        train_file[\"timestamp\"] = train_file.index\n",
    "\n",
    "        label_file = labels.doc[labels[\"trace_id\"] == filename, :]\n",
    "\n",
    "        for i in label_file.index:\n",
    "            ano_id = label_file.loc[i, \"anno_id\"]\n",
    "            selection_ref = train_file.loc[\n",
    "                (train_file[\"timestamp\"] >= label_file[\"ref_start\"][i])\n",
    "                & (train_file[\"timestamp\"] < label_file[\"ref_end\"][i]),\n",
    "                :,\n",
    "            ].copy()\n",
    "            selection_ref[\"ano_id\"] = ano_id\n",
    "            selection_ref[\"type_data\"] = 0\n",
    "            selection_ano = train_file.loc[\n",
    "                (train_file[\"timestamp\"] >= label_file[\"ano_start\"][i])\n",
    "                & (train_file[\"timestamp\"] <= label_file[\"ano_end\"][i]),\n",
    "                :,\n",
    "            ].copy()\n",
    "            selection_ano[\"ano_id\"] = ano_id\n",
    "            selection_ano[\"type_data\"] = 1\n",
    "            references[\n",
    "                f\"{from_files_to_anomaly_type[filename]}_{filename}_{i}\"\n",
    "            ] = selection_ref\n",
    "            anomalies[\n",
    "                f\"{from_files_to_anomaly_type[filename]}_{filename}_{i}\"\n",
    "            ] = selection_ano\n",
    "\n",
    "    assert references.keys() == anomalies.keys()\n",
    "    references = pd.concat(references).droplevel(1)\n",
    "    anomalies = pd.concat(anomalies).droplevel(1)\n",
    "\n",
    "    return references, anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ici il faut rajouter le code qui appelle les fonctions précédentes pour obtenir refs anos etc\n",
    "\n",
    "bursty_refs = refs[refs.index.str.startswith(\"bursty\")]\n",
    "\n",
    "\n",
    "bursty_anos = anos[anos.index.str.startswith(\"bursty\")]\n",
    "\n",
    "\n",
    "stalled_refs = refs[refs.index.str.startswith(\"stalled\")]\n",
    "\n",
    "\n",
    "stalled_anos = anos[anos.index.str.startswith(\"stalled\")]\n",
    "\n",
    "\n",
    "cpu_refs = refs[refs.index.str.startswith(\"CPU\")]\n",
    "\n",
    "\n",
    "cpu_anos = anos[anos.index.str.startswith(\"CPU\")]\n",
    "\n",
    "\n",
    "\n",
    "bursty_refs.shape, bursty_anos.shape, stalled_refs.shape, stalled_anos.shape, cpu_refs.shape, cpu_anos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_entropy(nb_ts_a: list, nb_ts_r: list) -> float:\n",
    "    \"\"\"Calculate the class entropy of a feature, which is the information\n",
    "    needed to describe the class distributions between two time serie.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nb_ts_a : list\n",
    "        Number of observations inside a time series belonging to the abnormal class.\n",
    "    nb_ts_r : list\n",
    "        Number of observations inside a time series belonging to the reference class.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The class entropy.\n",
    "    \"\"\"\n",
    "    if nb_ts_a == 0 or nb_ts_r == 0:\n",
    "        raise ValueError(\n",
    "            f\"One of the time series is empty. Len of TSA is {nb_ts_a} and len of TSR is {nb_ts_r}.\"\n",
    "        )\n",
    "    p_a = nb_ts_a / (nb_ts_a + nb_ts_r)\n",
    "    p_r = nb_ts_r / (nb_ts_a + nb_ts_r)\n",
    "    h_class = p_a * np.log2(p_a) + p_r * np.log2(p_r)\n",
    "    return h_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_observations_if_duplicates(\n",
    "    sorted_values: pd.DataFrame, feature\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Shuffle the observations if there are duplicates in the sorted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sorted_values : pd.DataFrame\n",
    "        The sorted values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The sorted values with shuffled values for duplicates.\n",
    "    \"\"\"\n",
    "\n",
    "    # On récupère le nombre de références et d'anomalies pour chaque modalité\n",
    "    value_type_to_count = sorted_values.groupby(feature).value_counts().to_dict()\n",
    "\n",
    "    # On récupère le nombre de valeurs distinctes pour chaque modalité (soit 1 lorsque pas de doublons, soit 2, lorsqu'il y a des doublons)\n",
    "    value_to_count_distinct = (\n",
    "        sorted_values.drop_duplicates().groupby(feature).count().to_dict()[\"type_data\"]\n",
    "    )\n",
    "\n",
    "    # On récupère les modalités distinctes (les différentes valeurs prises par la feature)\n",
    "    modalities = set(sorted_values[feature].tolist())\n",
    "    # En fait, ce qui est un peu bizarre c'est qu'on a des valeurs continues, mais là on va les traiter comme des valeurs discrètes :\n",
    "    # Par exemple si on considère une colonne qui prend les valeurs 501.03, 501.03, 502.4, 502.4, 505.0, on itère sur 501.03, 502.4, 505.0\n",
    "\n",
    "    # On parcourt chaque modalité\n",
    "    for modality in modalities:\n",
    "        # On récupère le premier type de données observé (ano ou ref, donc 1 ou 0) (ça nous sera utile plus tard)\n",
    "        last_type_data = sorted_values.loc[\n",
    "            sorted_values[feature] == modality, \"type_data\"\n",
    "        ].tolist()[0]\n",
    "\n",
    "        # Cas où il n'y a pas de doublons\n",
    "        if value_to_count_distinct[modality] == 1:\n",
    "            # On ne fait rien\n",
    "            continue\n",
    "\n",
    "        # Cas où il y a des doublons\n",
    "        else:\n",
    "            # On va shuffle dans le pire ordre possible\n",
    "\n",
    "            # D'abord on instancie les variables nécessaires\n",
    "            list_values = []\n",
    "            nb_refs = value_type_to_count[(modality, 0)]\n",
    "            nb_anos = value_type_to_count[(modality, 1)]\n",
    "            nb_total = nb_refs + nb_anos\n",
    "            # Hop maintenant c'est parti pour le shuffle\n",
    "\n",
    "            # Cas où il n'y a pas le même nombre de références et d'anomalies (cas le plus chiant)\n",
    "            if nb_refs != nb_anos:\n",
    "                # On instancie de nouveau des variables nécessaires\n",
    "                biggest = int(\n",
    "                    nb_refs < nb_anos\n",
    "                )  # 1 si on a plus d'anomalies que de références, 0 sinon\n",
    "                smallest = int(\n",
    "                    nb_refs > nb_anos\n",
    "                )  # 1 si on a plus de références que d'anomalies, 0 sinon\n",
    "                nb_smallest = min(\n",
    "                    nb_refs, nb_anos\n",
    "                )  # Nombre de fois où on va mettre la valeur la moins représentée\n",
    "\n",
    "                # On commence par mettre la valeur la plus représentée partout\n",
    "                list_values = [biggest] * nb_total\n",
    "\n",
    "                # Puis on cherche si le dernier type de donnée observé est le plus représenté ou le moins représenté pour savoir où commencer\n",
    "                start_smallest = 0 if smallest != last_type_data else 1\n",
    "\n",
    "                # On parcourt la liste 2 par 2 pour mettre la valeur la moins représentée\n",
    "                for i in range(start_smallest, nb_smallest * 2, 2):\n",
    "                    list_values[i] = smallest\n",
    "\n",
    "            # Cas où il y a le même nombre de références et d'anomalies (cas le plus simple)\n",
    "            else:\n",
    "                # On parcourt le nombre total d'observations\n",
    "                for i in range(nb_total):\n",
    "                    # On alterne entre 0 et 1 en commençant par la valeur opposée à la dernière valeur observée\n",
    "                    list_values.append(abs(last_type_data - i % 2 - 1))\n",
    "\n",
    "            # On récupère le dernier type de donnée observé (toujours 1 ou 0)\n",
    "            last_type_data = sorted_values.loc[\n",
    "                sorted_values[feature] == modality, \"type_data\"\n",
    "            ].tolist()[-1]\n",
    "\n",
    "        # On vérifie que la longueur de la liste est bien égale au nombre d'observations pour la modalité\n",
    "        assert (\n",
    "            len(list_values)\n",
    "            == sorted_values[sorted_values[feature] == modality].shape[0]\n",
    "        ), f\"Len of list_values {len(list_values)} is not equal to the number of observations for the modality {sorted_values[sorted_values[feature]==modality].shape[0]}.\"\n",
    "        # On met à jour le type de donnée pour la modalité\n",
    "        sorted_values.loc[sorted_values[feature] == modality, \"type_data\"] = list_values\n",
    "\n",
    "    return sorted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_entropy(shuffled_values: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the segmentation entropy of a feature.\n",
    "\n",
    "    Parameters:\n",
    "    shuffled_values (pd.DataFrame): A DataFrame containing the shuffled values.\n",
    "\n",
    "    Returns:\n",
    "    float: The segmentation entropy of the feature.\n",
    "    \"\"\"\n",
    "    # On récupère la time serie\n",
    "    ts = shuffled_values[\"type_data\"].tolist()\n",
    "\n",
    "    # Stocke la première valeur de la liste\n",
    "    past_value = ts[0]\n",
    "\n",
    "    # Liste pour stocker les valeurs à l'intérieur d'un segment\n",
    "    values_inside_segment = []\n",
    "\n",
    "    # Variable pour stocker la segmentation entropy\n",
    "    segmentation_ent = 0.0\n",
    "\n",
    "    # Parcourt chaque valeur dans la time serie\n",
    "    for value in ts:\n",
    "        # Si la valeur est différente de la valeur précédente\n",
    "        if value != past_value:\n",
    "            # On a un nouveau segment, il faut calculer l'entropie de segmentation partielle du précédent segment\n",
    "            pi = len(values_inside_segment) / shuffled_values.shape[0]\n",
    "            segmentation_ent += pi * np.log(1 / pi)\n",
    "\n",
    "            # On réinitialise la liste des valeurs à l'intérieur du segment avec la nouvelle valeur\n",
    "            values_inside_segment = [value]\n",
    "        else:\n",
    "            # On stocke les valeurs à l'intérieur du segment tant qu'un nouveau segment n'est pas créé\n",
    "            values_inside_segment.append(value)\n",
    "\n",
    "        # On met à jour la valeur précédente avec la valeur actuelle\n",
    "        past_value = value\n",
    "\n",
    "    return segmentation_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_reward_function(refs, anos):\n",
    "    \"\"\"\n",
    "    Calculates the reward function for a single feature based on the reference data and the annotated data.\n",
    "\n",
    "    Parameters:\n",
    "    refs (pandas.DataFrame): The reference data.\n",
    "    anos (pandas.DataFrame): The abnormal data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the single reward function for each feature.\n",
    "    \"\"\"\n",
    "    distances = {}\n",
    "    # On calcule la class entropy\n",
    "    class_ent = class_entropy(refs.shape[0], anos.shape[0])\n",
    "    # On calcule la segmentation entropy pour chaque feature sauf type_data\n",
    "    for feature in [col for col in refs.columns if col != \"type_data\"]:\n",
    "        # On concatène les références et les anomalies pour la feature\n",
    "        all_values = pd.concat(\n",
    "            [refs[[feature, \"type_data\"]], anos[[feature, \"type_data\"]]]\n",
    "        )\n",
    "        # On trie les valeurs par feature puis par type_data\n",
    "        sorted_values = all_values.sort_values(by=[feature, \"type_data\"])\n",
    "        # On shuffle les valeurs si on a des doublons\n",
    "        shuffled_values = shuffle_observations_if_duplicates(sorted_values, feature)\n",
    "        # On calcule la segmentation entropy\n",
    "        segmentation_ent = segmentation_entropy(shuffled_values)\n",
    "        # On calcule la single reward function\n",
    "        distance = class_ent / segmentation_ent\n",
    "        # On stocke la single reward function dans le dictionnaire\n",
    "        distances[feature] = distance\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def single_reward_function_check(refs, anos):\n",
    "    distances = {}\n",
    "    segmentations_ents = {}\n",
    "    class_ent = class_entropy(refs.shape[0], anos.shape[0])\n",
    "    for feature in refs.columns[:-4]:\n",
    "        all_values = pd.concat([refs[[feature, \"type_data\"]], anos[[feature, \"type_data\"]]])\n",
    "        sorted_values = all_values.sort_values(by=[feature, \"type_data\"])\n",
    "        shuffled_values = shuffle_observations_if_duplicates(sorted_values, feature)\n",
    "        segmentation_ent = segmentation_entropy(shuffled_values)\n",
    "        distance = class_ent / segmentation_ent\n",
    "        distances[feature] = distance\n",
    "        segmentations_ents[feature] = segmentation_ent\n",
    "    return distances, segmentations_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursty_df = pd.concat([bursty_refs, bursty_anos])  # .iloc[:, :-4]\n",
    "stalled_df = pd.concat([stalled_refs, stalled_anos])  # .iloc[:, :-4]\n",
    "cpu_df = pd.concat([cpu_refs, cpu_anos])  # .iloc[:, :-4]\n",
    "\n",
    "dfs = [bursty_df, stalled_df, cpu_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(df, correlation_threshold=0.9):\n",
    "    # Step 1: Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Step 2: Create a graph based on pairwise correlations\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes (features) to the graph\n",
    "    G.add_nodes_from(correlation_matrix.columns)\n",
    "\n",
    "    # Add edges between nodes if the correlation exceeds a threshold\n",
    "    for i in range(len(correlation_matrix.columns[:-4])):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "                G.add_edge(correlation_matrix.columns[i], correlation_matrix.columns[j])\n",
    "\n",
    "    # Step 3: Extract clusters from the graph\n",
    "    clusters = list(nx.connected_components(G))\n",
    "\n",
    "    # Step 4: Select one representative feature from each cluster\n",
    "    selected_features = [cluster.pop() for cluster in clusters]\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features = []\n",
    "\n",
    "for df in dfs:\n",
    "    filtered_features.append(remove_correlated_features(df))\n",
    "\n",
    "print(\", \".join([str(len(feature)) for feature in filtered_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_filtering(refs, anos, cols):\n",
    "    new_cols = []\n",
    "    refs_df = refs[cols]\n",
    "    anos_df = anos[cols]\n",
    "    cols_to_visit = list(anos_df.columns[:-4])\n",
    "    for ano in anos_df.index.unique():\n",
    "        for col in cols_to_visit:\n",
    "            pattern = anos_df.loc[ano, col]\n",
    "            ts = refs_df.loc[:, col]\n",
    "            matches = stumpy.match(pattern, ts, max_distance=28.0)\n",
    "            if not list(matches):\n",
    "                if col not in new_cols:\n",
    "                    new_cols.append(col)\n",
    "            else:\n",
    "                cols_to_visit.remove(col)\n",
    "                print(f\"Found {len(matches)} match(es) for {col} in ano {ano}\")\n",
    "\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_features_bursty = false_positive_filtering(\n",
    "    bursty_refs, bursty_anos, filtered_features[0]\n",
    ")\n",
    "new_filtered_features_stalled = false_positive_filtering(\n",
    "    stalled_refs, stalled_anos, filtered_features[1]\n",
    ")\n",
    "new_filtered_features_cpu = false_positive_filtering(\n",
    "    cpu_refs, cpu_anos, filtered_features[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_features = [\n",
    "    new_filtered_features_cpu,\n",
    "    new_filtered_features_bursty,\n",
    "    new_filtered_features_stalled,\n",
    "]\n",
    "refs_anos = [\n",
    "    (bursty_refs, bursty_anos),\n",
    "    (stalled_refs, stalled_anos),\n",
    "    (cpu_refs, cpu_anos),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward leap filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate single reward function for each column"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "distances = []\n",
    "new_refs_anos = []\n",
    "\n",
    "for ref_ano, new_filtered_feature in zip(refs_anos, new_filtered_features):\n",
    "    ref, ano = ref_ano\n",
    "    ref = ref[new_filtered_feature + [\"type_data\"]]\n",
    "    ano = ano[new_filtered_feature + [\"type_data\"]]\n",
    "    distances.append(single_reward_function(ref, ano))\n",
    "    new_refs_anos.append((ref, ano))\n",
    "\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai laissé le code juste en dessous car c'est probablement celui que toi tu as utilisé, j'ai voulu \"réduire\" à l'aide d'une boucle for mais choisi ce qui te convient le mieux entre la ligne de code au dessus et celle en dessous !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_bursty = single_reward_function(\n",
    "    bursty_refs.loc[:, new_filtered_features_bursty + [\"type_data\"]],\n",
    "    bursty_anos.loc[:, new_filtered_features_bursty + [\"type_data\"]],\n",
    ")\n",
    "\n",
    "\n",
    "distances_stalled = single_reward_function(\n",
    "    stalled_refs.loc[:, new_filtered_features_stalled + [\"type_data\"]],\n",
    "    stalled_anos.loc[:, new_filtered_features_stalled + [\"type_data\"]],\n",
    ")\n",
    "\n",
    "\n",
    "distances_cpu = single_reward_function(\n",
    "    cpu_refs.loc[:, new_filtered_features_cpu + [\"type_data\"]],\n",
    "    cpu_anos.loc[:, new_filtered_features_cpu + [\"type_data\"]],\n",
    ")\n",
    "\n",
    "distances_bursty, distances_stalled, distances_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifications (not needed inside reel code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref_ano, new_filtered_feature in zip(refs_anos, new_filtered_features):\n",
    "    ref, ano = ref_ano\n",
    "    ref = ref[new_filtered_feature + [\"type_data\"]]\n",
    "    ano = ano[new_filtered_feature + [\"type_data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 3, figsize=(10, 15))\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i, ref_ano in enumerate(refs_anos):\n",
    "    ref, ano = ref_ano\n",
    "    for j in range(len(new_filtered_features[i])):\n",
    "        axs[i + j].hist(\n",
    "            ref[new_filtered_features[i][j]].astype(float).tolist(),\n",
    "            label=\"Reference\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        axs[i + j].hist(\n",
    "            ano[new_filtered_features[i][j]].astype(float).tolist(),\n",
    "            label=\"Anomaly\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "        axs[i + j].set_title(new_filtered_features[i][j], fontsize=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    int(len(refs.columns[:-4]) / 2), 2, figsize=(10, 2 * len(refs.columns[:-4]))\n",
    ")\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i, item in enumerate(sorted(distances.items(), key=lambda x: x[1])):\n",
    "    col = item[0]\n",
    "    axs[i].hist(refs[col].astype(float).tolist(), label=\"Reference\", alpha=0.5)\n",
    "    axs[i].hist(anos[col].astype(float).tolist(), label=\"Anomaly\", alpha=0.5)\n",
    "    axs[i].ticklabel_format(style=\"plain\")\n",
    "\n",
    "    axs[i].set_title(col, fontsize=6)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting absolute values and sorting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_and_sort_distances(distances: dict) -> dict:\n",
    "    if all(value < 0 for value in distances.values()):\n",
    "        positive_distances = {key: np.abs(value) for key, value in distances.items()}\n",
    "\n",
    "    sorted_distances = dict(\n",
    "        sorted(positive_distances.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    return sorted_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_bursty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_and_sort_distances(distances_bursty), absolute_and_sort_distances(\n",
    "    distances_stalled\n",
    "), absolute_and_sort_distances(distances_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.24461719290681788 - 0.11803854429547568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_leap(distances: dict) -> float:\n",
    "    ranked_distances = absolute_and_sort_distances(distances)\n",
    "\n",
    "    leaps = [\n",
    "        last_distance - distance\n",
    "        for last_distance, distance in zip(\n",
    "            ranked_distances.values(), list(ranked_distances.values())[1:]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    maximum_leap = max(leaps)\n",
    "\n",
    "    return maximum_leap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_leap(distances_bursty), maximum_leap(distances_stalled), maximum_leap(\n",
    "    distances_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_leap_filter(distances: dict) -> dict:\n",
    "    ranked_distances = absolute_and_sort_distances(distances)\n",
    "    threshold = maximum_leap(distances)\n",
    "    to_be_discarded = set()\n",
    "\n",
    "    last_distance = 0\n",
    "    for feature, distance in ranked_distances.items():\n",
    "        if last_distance != 0:\n",
    "            leap = last_distance - distance\n",
    "            if leap <= threshold:\n",
    "                to_be_discarded.update([feature])\n",
    "        last_distance = distance\n",
    "\n",
    "    filtered_features = {\n",
    "        feature: distance\n",
    "        for feature, distance in distances.items()\n",
    "        if feature not in to_be_discarded\n",
    "    }\n",
    "\n",
    "    return filtered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_leap_filter(distances_bursty), reward_leap_filter(\n",
    "    distances_stalled\n",
    "), reward_leap_filter(distances_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visual Checking\n",
    "##### Bursty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    absolute_and_sort_distances(distances_bursty).keys(),\n",
    "    absolute_and_sort_distances(distances_bursty).values(),\n",
    ")\n",
    "plt.axhline(\n",
    "    y=np.median(list(absolute_and_sort_distances(distances_bursty).values())), c=\"r\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming distances_bursty, absolute_and_sort_distances, and reward_leap_filter are defined\n",
    "\n",
    "# Get distances and their absolute values\n",
    "distances = absolute_and_sort_distances(distances_bursty)\n",
    "\n",
    "# Get filtered distances\n",
    "filtered_distances = reward_leap_filter(distances_bursty)\n",
    "\n",
    "# Extract feature names and distances for plotting\n",
    "feature_names = list(distances.keys())\n",
    "all_distances = list(distances.values())\n",
    "\n",
    "# Set color for bars\n",
    "colors = [\n",
    "    \"orange\" if feature in filtered_distances else \"b\" for feature in feature_names\n",
    "]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.bar(feature_names, all_distances, color=colors)\n",
    "\n",
    "# Add a red horizontal line at the median\n",
    "plt.axhline(y=np.median(all_distances), color=\"r\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Stalled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    absolute_and_sort_distances(distances_stalled).keys(),\n",
    "    absolute_and_sort_distances(distances_stalled).values(),\n",
    ")\n",
    "plt.axhline(\n",
    "    y=np.median(list(absolute_and_sort_distances(distances_stalled).values())), c=\"r\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming distances_bursty, absolute_and_sort_distances, and reward_leap_filter are defined\n",
    "\n",
    "# Get distances and their absolute values\n",
    "distances = absolute_and_sort_distances(distances_stalled)\n",
    "\n",
    "# Get filtered distances\n",
    "filtered_distances = reward_leap_filter(distances_stalled)\n",
    "\n",
    "# Extract feature names and distances for plotting\n",
    "feature_names = list(distances.keys())\n",
    "all_distances = list(distances.values())\n",
    "\n",
    "# Set color for bars\n",
    "colors = [\n",
    "    \"orange\" if feature in filtered_distances else \"b\" for feature in feature_names\n",
    "]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.bar(feature_names, all_distances, color=colors)\n",
    "\n",
    "# Add a red horizontal line at the median\n",
    "plt.axhline(y=np.median(all_distances), color=\"r\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    absolute_and_sort_distances(distances_cpu).keys(),\n",
    "    absolute_and_sort_distances(distances_cpu).values(),\n",
    ")\n",
    "plt.axhline(\n",
    "    y=np.median(list(absolute_and_sort_distances(distances_cpu).values())), c=\"r\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming distances_bursty, absolute_and_sort_distances, and reward_leap_filter are defined\n",
    "\n",
    "# Get distances and their absolute values\n",
    "distances = absolute_and_sort_distances(distances_cpu)\n",
    "\n",
    "# Get filtered distances\n",
    "filtered_distances = reward_leap_filter(distances_cpu)\n",
    "\n",
    "# Extract feature names and distances for plotting\n",
    "feature_names = list(distances.keys())\n",
    "all_distances = list(distances.values())\n",
    "\n",
    "# Set color for bars\n",
    "colors = [\n",
    "    \"orange\" if feature in filtered_distances else \"b\" for feature in feature_names\n",
    "]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.bar(feature_names, all_distances, color=colors)\n",
    "\n",
    "# Add a red horizontal line at the median\n",
    "plt.axhline(y=np.median(all_distances), color=\"r\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method\n",
    "\n",
    "Là c'est une fonction que j'avais fait en attendant d'avoir ta fonction, je récupérais juste les 3 meilleures colonnes ! Je vais remodifier pour l'adapter à ton code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for distance, new_ref_ano in zip(distances, new_refs_anos):\n",
    "    top_cols = sorted(distance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    top_cols = [col[0] for col in top_cols]\n",
    "    ref, ano = new_ref_ano\n",
    "    ref = ref[top_cols + [\"type_data\"]]\n",
    "    ano = ano[top_cols + [\"type_data\"]]\n",
    "    for feature in [col for col in ref.columns if col != \"type_data\"]:\n",
    "        # On concatène les références et les anomalies pour la feature\n",
    "        all_values = pd.concat(\n",
    "            [ref.loc[:, [feature, \"type_data\"]], ano.loc[:, [feature, \"type_data\"]]]\n",
    "        )\n",
    "\n",
    "        # On trie les valeurs par feature puis par type_data\n",
    "        sorted_values = all_values.sort_values(by=[feature, \"type_data\"])\n",
    "\n",
    "        # On cherche les valeurs \"boundary\" (les valeurs qui séparent les anomalies des références)\n",
    "        boundaries = []\n",
    "        for i in range(sorted_values.shape[0] - 1):\n",
    "            if sorted_values.iloc[i, 1] != sorted_values.iloc[i + 1, 1]:\n",
    "                if sorted_values.iloc[i, 0] not in boundaries:\n",
    "                    boundaries.append(sorted_values.iloc[i, 0])\n",
    "\n",
    "        print(len(boundaries))\n",
    "        print(boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.loc[:, [feature, \"type_data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values[\"type_data\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing final explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursty_anos.loc[:, new_filtered_features_bursty + [\"type_data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in train_files:\n",
    "    ano_ids = list(labels.loc[labels[\"trace_id\"] == file, \"ano_id\"])\n",
    "    for ano_id in ano_ids:\n",
    "        selected_ref = refs.loc[\n",
    "            (refs[\"ano_id\"] == ano_id) & (refs[\"original_file\"] == file), :\n",
    "        ]\n",
    "        selected_ano = anos.loc[\n",
    "            (anos[\"ano_id\"] == ano_id) & (anos[\"original_file\"] == file), :\n",
    "        ]\n",
    "\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
